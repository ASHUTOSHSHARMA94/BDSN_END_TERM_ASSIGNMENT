{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r7Ix40LM83j"
      },
      "source": [
        "# A quick summary\n",
        "\n",
        "* Import Libraries\n",
        "* Build Spark Session\n",
        "* Data Load\n",
        "* Data Exploration & Preparation\n",
        "* Feature Engineering\n",
        "* Data Scaling\n",
        "* Data Split\n",
        "* Build, Train & Evaluate Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Spark Python\n",
        "\n",
        "**apt-get update. upgrade :** \n",
        "\n",
        "This command is used to install the latest versions of the packages currently installed on the user's system. The installed packages which have new packages available are retrieved and installed.\n",
        "\n",
        "**!apt install openjdk-8-jdk-headless -qq > /dev/null**\n",
        "\n",
        "latest java verion was being installed."
      ],
      "metadata": {
        "id": "R52Ck0JDNSA6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYzG59RKM83o",
        "outputId": "5a0a98b9-f28d-4403-c24e-8452f94326fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!apt update > /dev/null\n",
        "!apt install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This coding will help us to install latest verion of spark in google colab."
      ],
      "metadata": {
        "id": "NA1Avx6JGEQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q http://apache.osuosl.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
        "\n",
        "\n",
        "!tar xf spark-3.1.2-bin-hadoop3.2.tgz\n",
        "\n",
        "!pip install -q pyspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "wIzrIbf1VAel"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "new spark session has been activated and we are using port number 4050 in the code"
      ],
      "metadata": {
        "id": "otYPq7giGFI5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPwrsQAcM83p"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "pOC4y5KnM83q"
      },
      "outputs": [],
      "source": [
        "#Generic Libraries\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "#Apache Spark Libraries\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col,regexp_replace\n",
        "\n",
        "#Apache Spark ML CLassifier Libraries\n",
        "from pyspark.ml.classification import DecisionTreeClassifier,RandomForestClassifier,NaiveBayes\n",
        "\n",
        "#Apache Spark Evaluation Library\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "#Apache Spark Features libraries\n",
        "from pyspark.ml.feature import StandardScaler,StringIndexer, VectorAssembler, VectorIndexer, OneHotEncoder\n",
        "\n",
        "#Apache Spark Pipelin Library\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Apache Spark `DenseVector`\n",
        "from pyspark.ml.linalg import DenseVector\n",
        "\n",
        "#Data Split Libraries\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "#Tabulating Data\n",
        "from tabulate import tabulate\n",
        "\n",
        "#Garbage\n",
        "import gc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "all the necessary libraries required for this dataset have been written above."
      ],
      "metadata": {
        "id": "5tiZ-BHoGp6D"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxLT2De2M83r"
      },
      "source": [
        "## Build Spark Session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Fbfqfk3yM83r"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = (SparkSession.builder\n",
        "                  .appName('Apache Spark Beginner Tutorial')\n",
        "                  .config(\"spark.executor.memory\", \"1G\")\n",
        "                  .config(\"spark.executor.cores\",\"4\")\n",
        "                  .getOrCreate())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "new spark session has been activated."
      ],
      "metadata": {
        "id": "JXIjP761HEfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sparkContext.setLogLevel('INFO')"
      ],
      "metadata": {
        "id": "zJ5CNTOl20D2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Mr0U-q50M83s",
        "outputId": "08fb10ed-88f6-4d9b-ec5c-0035350a6bf6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'3.1.2'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "spark.version"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It will help us to show which verion of spark are we using"
      ],
      "metadata": {
        "id": "fY5Uk1ZqHQMY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYAHkbKnM83u"
      },
      "source": [
        "## Data Load"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
      ],
      "metadata": {
        "id": "B9x7ab44tlPk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are taking the dataset iris dataset by the name iris.data\n",
        "\n",
        "wget will help us to take data from the server."
      ],
      "metadata": {
        "id": "i07clyY8HuAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_file = \"./iris.data\""
      ],
      "metadata": {
        "id": "LC6luuPW69tA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "New name have been assigned to dataset called data_file"
      ],
      "metadata": {
        "id": "dQBe10mHHv5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = spark.read.format(\"csv\") \\\n",
        "       .option(\"header\", \"true\") \\\n",
        "       .option(\"inferSchema\",\"true\")\\\n",
        "       .load(data_file) "
      ],
      "metadata": {
        "id": "KQh_0TfNkkJ2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This coding will help to show the data in SQL format"
      ],
      "metadata": {
        "id": "2EqsNUKWIEos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.withColumnRenamed(\"5.1\",\"sepal_length\").withColumnRenamed(\"3.5\",\"sepal_width\").withColumnRenamed(\"1.4\",\"petal_length\").withColumnRenamed(\"0.2\",\"petal_width\").withColumnRenamed(\"Iris-setosa\",\"species\")"
      ],
      "metadata": {
        "id": "oJJbmAg9_Yns"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changing the name of all the columns given in the Dataset"
      ],
      "metadata": {
        "id": "L8aV5mSLIKt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9o4MgCMF2_xh",
        "outputId": "3a382fb3-b44f-4abe-8e52-10dfe1778ad2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[sepal_length: double, sepal_width: double, petal_length: double, petal_width: double, species: string]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "cache will help us in the faster reuse of the data as it get stored in the memory"
      ],
      "metadata": {
        "id": "mStY9S8gIZ1Q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo2zHiwMM83v"
      },
      "source": [
        "## Data Exploration & Preparation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Total records \n",
        "data.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YclKwlq5FN24",
        "outputId": "540564da-89aa-4ae2-ff1c-b977c4c8caf1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "149"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "with the help of count we are able to check the to data avaiable in our dataset"
      ],
      "metadata": {
        "id": "Zn3Y6I5rJHaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nc0vd9VjFN5e",
        "outputId": "475c0a68-975e-4e7e-bd46-8732a56b0875"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+------------+-----------+-----------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|    species|\n",
            "+------------+-----------+------------+-----------+-----------+\n",
            "|         4.9|        3.0|         1.4|        0.2|Iris-setosa|\n",
            "|         4.7|        3.2|         1.3|        0.2|Iris-setosa|\n",
            "|         4.6|        3.1|         1.5|        0.2|Iris-setosa|\n",
            "|         5.0|        3.6|         1.4|        0.2|Iris-setosa|\n",
            "|         5.4|        3.9|         1.7|        0.4|Iris-setosa|\n",
            "+------------+-----------+------------+-----------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we checking the top 5 data"
      ],
      "metadata": {
        "id": "9mUob5gJJRfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.withColumn(\"species\",regexp_replace(data[\"species\"],\"Iris-\",\"\"))"
      ],
      "metadata": {
        "id": "D3V963DdFN8D"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are able replace the Iris- with the space with help of regexp_replace"
      ],
      "metadata": {
        "id": "enNQyxNCJWGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxh3M3OJFN_Y",
        "outputId": "2f9bca48-8ded-4fcd-e0ce-a718c858a288"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+------------+-----------+-------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
            "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
            "|         4.6|        3.1|         1.5|        0.2| setosa|\n",
            "|         5.0|        3.6|         1.4|        0.2| setosa|\n",
            "|         5.4|        3.9|         1.7|        0.4| setosa|\n",
            "+------------+-----------+------------+-----------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here again we are checking the top 5 data"
      ],
      "metadata": {
        "id": "yRe3IxVMJqke"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXBlqsqeM83x",
        "outputId": "c52cf348-4ce9-44cb-d920-e5e75372cab2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+\n",
            "|   species|count|\n",
            "+----------+-----+\n",
            "| virginica|   50|\n",
            "|versicolor|   50|\n",
            "|    setosa|   49|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Records per Species\n",
        "data.groupBy('species').count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are using the groupby function in our to evaluate the to species per categories"
      ],
      "metadata": {
        "id": "vF1Hm4MSJuKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8QPAhbZKFN5",
        "outputId": "223153c5-4f50-4098-acc9-31f7f5552011"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+------------------+------------------+------------------+---------+------------------+\n",
            "|summary|      sepal_length|       sepal_width|      petal_length|       petal_width|  species|      species_indx|\n",
            "+-------+------------------+------------------+------------------+------------------+---------+------------------+\n",
            "|  count|               149|               149|               149|               149|      149|               149|\n",
            "|   mean| 5.848322147651008| 3.051006711409397|3.7744966442953043|1.2053691275167793|     null|0.9932885906040269|\n",
            "| stddev|0.8285940572656166|0.4334988777167475|1.7596511617753412|0.7612920413899604|     null|0.8178469120551444|\n",
            "|    min|               4.3|               2.0|               1.0|               0.1|   setosa|               0.0|\n",
            "|    max|               7.9|               4.4|               6.9|               2.5|virginica|               2.0|\n",
            "+-------+------------------+------------------+------------------+------------------+---------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD1OenYFM83z"
      },
      "source": [
        "Inorder for our model to make predictions the Species aka Label column should be a numerical value (models don't like string!). To achieve this we shall use String Indexing on the Species columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "je9qi4aRM83z",
        "outputId": "74713e92-3e6f-47c0-9453-72ff47e6d1ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+------------+-----------+-------+------------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|species|species_indx|\n",
            "+------------+-----------+------------+-----------+-------+------------+\n",
            "|         4.9|        3.0|         1.4|        0.2| setosa|         2.0|\n",
            "|         4.7|        3.2|         1.3|        0.2| setosa|         2.0|\n",
            "|         4.6|        3.1|         1.5|        0.2| setosa|         2.0|\n",
            "|         5.0|        3.6|         1.4|        0.2| setosa|         2.0|\n",
            "|         5.4|        3.9|         1.7|        0.4| setosa|         2.0|\n",
            "+------------+-----------+------------+-----------+-------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#String Indexing the Species column\n",
        "SIndexer = StringIndexer(inputCol='species', outputCol='species_indx')\n",
        "data = SIndexer.fit(data).transform(data)\n",
        "\n",
        "#Inspect the dataset\n",
        "data.show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "String indexing was being done using StringIndexer function. After that we fit the data and transform it"
      ],
      "metadata": {
        "id": "sSpsZbxuKP2L"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdJ2BWoOM830"
      },
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxP78yh7M830"
      },
      "source": [
        "The Spark model needs two columns: “label” and “features” and we are not going to do much feature engineering because we want to focus on the mechanics of training the model in Spark. \n",
        "\n",
        "So, creating a seperate dataframe with re-ordered columns, then defining an input data using Dense Vector. A Dense Vector is a local vector that is backed by a double array that represents its entry values. In other words, it's used to store arrays of values for use in PySpark.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "JaR4t-AwM83_",
        "outputId": "28a46266-5927-4e4b-e834-185c6c943b8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+------------+-----------+------------+-----------+\n",
            "|species_indx|sepal_length|sepal_width|petal_length|petal_width|\n",
            "+------------+------------+-----------+------------+-----------+\n",
            "|         2.0|         4.9|        3.0|         1.4|        0.2|\n",
            "|         2.0|         4.7|        3.2|         1.3|        0.2|\n",
            "|         2.0|         4.6|        3.1|         1.5|        0.2|\n",
            "|         2.0|         5.0|        3.6|         1.4|        0.2|\n",
            "|         2.0|         5.4|        3.9|         1.7|        0.4|\n",
            "+------------+------------+-----------+------------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#creating a seperate dataframe with re-ordered columns\n",
        "df = data.select(\"species_indx\",\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\")\n",
        "\n",
        "#Inspect the dataframe\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiVzG5ACM84A"
      },
      "source": [
        "**Note:** Observe that the species column which is our label (aka Target) is now at beginning of the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "z6_lqACdM84B"
      },
      "outputs": [],
      "source": [
        "# Define the `input_data` as Dense Vector\n",
        "input_data = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jij5_Eu5M84B"
      },
      "source": [
        "**Note:** Observe the definition of the Dense Vector. So,when we create a new indexed dataframe(below) the machine understands that the first column is a Label (Target) and the remaining columns are Features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ZV575mfeM84C"
      },
      "outputs": [],
      "source": [
        "# Creating a new Indexed Dataframe\n",
        "df_indx = spark.createDataFrame(input_data, [\"label\", \"features\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "EIho0V2UM84D",
        "outputId": "94094f19-3a9a-4ddc-dec4-39c392f57ea1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------------+\n",
            "|label|         features|\n",
            "+-----+-----------------+\n",
            "|  2.0|[4.9,3.0,1.4,0.2]|\n",
            "|  2.0|[4.7,3.2,1.3,0.2]|\n",
            "|  2.0|[4.6,3.1,1.5,0.2]|\n",
            "|  2.0|[5.0,3.6,1.4,0.2]|\n",
            "|  2.0|[5.4,3.9,1.7,0.4]|\n",
            "+-----+-----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#view the indexed dataframe\n",
        "df_indx.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll_hWjg7M84E"
      },
      "source": [
        "## Data Scaling\n",
        "\n",
        "This is also known as Feature Scaling. It is a method of normalizing the features of the data. Scaling can make a difference between a weak machine learning model and a better one. \n",
        "\n",
        "In this tutorial we will use a Standard Scaler to scale our feature data. Apache Spark has a Standard Scaler library to do the job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "xx8uDMezM84F"
      },
      "outputs": [],
      "source": [
        "#Initialize Standard Scaler\n",
        "stdScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
        "\n",
        "#Fit the Standard Scaler to the indexed Dataframe\n",
        "scaler = stdScaler.fit(df_indx)\n",
        "\n",
        "#Transform the dataframe\n",
        "df_scaled =scaler.transform(df_indx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "KQIAEvUPM84F",
        "outputId": "a7db831c-0084-4dcc-c930-284932af6f30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------------+--------------------+\n",
            "|label|         features|     features_scaled|\n",
            "+-----+-----------------+--------------------+\n",
            "|  2.0|[4.9,3.0,1.4,0.2]|[5.91363159925396...|\n",
            "|  2.0|[4.7,3.2,1.3,0.2]|[5.67225888091706...|\n",
            "|  2.0|[4.6,3.1,1.5,0.2]|[5.55157252174862...|\n",
            "|  2.0|[5.0,3.6,1.4,0.2]|[6.03431795842241...|\n",
            "|  2.0|[5.4,3.9,1.7,0.4]|[6.51706339509620...|\n",
            "+-----+-----------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Viewing the Scaled Data\n",
        "df_scaled.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "jNxybtQjM84G"
      },
      "outputs": [],
      "source": [
        "#Dropping the Features column\n",
        "df_scaled = df_scaled.drop(\"features\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDMSRLsFM84G"
      },
      "source": [
        "## Data Split\n",
        "\n",
        "Just like always, before building a model we shall split our scaled dataset into training & test sets. \n",
        "Training Dataset = 90%\n",
        "Test Dataset = 10%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "6NfEIZ4DM84G"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = df_scaled.randomSplit([0.9, 0.1], seed = 12345)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Nwa7xkv9M84G",
        "outputId": "e6a30773-3ed3-4150-8c42-70a570afc899",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+\n",
            "|label|     features_scaled|\n",
            "+-----+--------------------+\n",
            "|  0.0|[5.91363159925396...|\n",
            "|  0.0|[6.03431795842241...|\n",
            "|  0.0|[6.03431795842241...|\n",
            "|  0.0|[6.15500431759086...|\n",
            "|  0.0|[6.27569067675931...|\n",
            "+-----+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Inspect Training Data\n",
        "train_data.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2fpLbaqM84H"
      },
      "source": [
        "## Build, Train & Evaluate Model\n",
        "\n",
        "In this step we will create multiple models, train them on our scaled dataset and then compare their accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "YhNWnTiaM84H"
      },
      "outputs": [],
      "source": [
        "model = ['Decision Tree','Random Forest','Naive Bayes']\n",
        "model_results = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "5Y9DLM_aM84H"
      },
      "outputs": [],
      "source": [
        "# -- Decision Tree Classifier --\n",
        "\n",
        "dtc = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features_scaled\")          #instantiate the model\n",
        "dtc_model = dtc.fit(train_data)                                                        #train the model\n",
        "dtc_pred = dtc_model.transform(test_data)                                              #model predictions\n",
        "\n",
        "#Evaluate the Model\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "dtc_acc = evaluator.evaluate(dtc_pred)\n",
        "#print(\"Decision Tree Classifier Accuracy =\", '{:.2%}'.format(dtc_acc))\n",
        "model_results.extend([[model[0],'{:.2%}'.format(dtc_acc)]])                               #appending to list\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "8tP4IYzMM84I"
      },
      "outputs": [],
      "source": [
        "# -- Random Forest Classifier --\n",
        "\n",
        "rfc = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features_scaled\", numTrees=10)          #instantiate the model\n",
        "rfc_model = rfc.fit(train_data)                                                                     #train the model\n",
        "rfc_pred = rfc_model.transform(test_data)                                                           #model predictions\n",
        "\n",
        "#Evaluate the Model\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "rfc_acc = evaluator.evaluate(rfc_pred)\n",
        "#print(\"Random Forest Classifier Accuracy =\", '{:.2%}'.format(rfc_acc))\n",
        "model_results.extend([[model[1],'{:.2%}'.format(rfc_acc)]])                                            #appending to list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "SWqK0vxrM84I"
      },
      "outputs": [],
      "source": [
        "# -- Naive Bayes Classifier --\n",
        "\n",
        "nbc = NaiveBayes(smoothing=1.0,modelType=\"multinomial\", labelCol=\"label\",featuresCol=\"features_scaled\")    #instantiate the model\n",
        "nbc_model = nbc.fit(train_data)                                                                          #train the model\n",
        "nbc_pred = nbc_model.transform(test_data)                                                                #model predictions\n",
        "\n",
        "#Evaluate the Model\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "nbc_acc = evaluator.evaluate(nbc_pred)\n",
        "#print(\"Naive Bayes Accuracy =\", '{:.2%}'.format(nbc_acc))\n",
        "model_results.extend([[model[2],'{:.2%}'.format(nbc_acc)]])                                            #appending to list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "collapsed": true,
        "id": "P1klHmyWM84J",
        "outputId": "5bdcb9a6-0207-4b61-846a-372cced1b8b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "503"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "#freeing memory\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZcd1ieRM84J"
      },
      "source": [
        "Tabulating the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "akLdrdldM84L",
        "outputId": "228e7864-83c3-4e15-c047-e2a7d2babeeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classifier Models    Accuracy\n",
            "-------------------  ----------\n",
            "Decision Tree        81.82%\n",
            "Random Forest        81.82%\n",
            "Naive Bayes          90.91%\n"
          ]
        }
      ],
      "source": [
        "print (tabulate(model_results, headers=[\"Classifier Models\", \"Accuracy\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From here we can observe that Naive Bayes is giving us the better result in machine learning as compared to the Decision tree and Random Forest"
      ],
      "metadata": {
        "id": "nVR8A4LVLWRw"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "Ashutosh_Sharma_A21006_End_Term_BDSN.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}